{
  "main": {
    "id": "2fe2383a4eee5a56",
    "type": "split",
    "children": [
      {
        "id": "46ca3c55bf81319e",
        "type": "tabs",
        "children": [
          {
            "id": "8ebf47b689d6a922",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "05_speech_recognition/(2021) W2V-BERT, Combining Contrastrive learning and Masked Language Modeling For Self-Supervised Speech Pre-Training.md",
                "mode": "source",
                "source": false
              }
            }
          },
          {
            "id": "9782c5ebcde6b4d5",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "05_speech_recognition/(2021) W2V-BERT, Combining Contrastrive learning and Masked Language Modeling For Self-Supervised Speech Pre-Training.md",
                "mode": "source",
                "source": false
              }
            }
          }
        ],
        "currentTab": 1
      }
    ],
    "direction": "vertical"
  },
  "left": {
    "id": "4de10ac94656e612",
    "type": "split",
    "children": [
      {
        "id": "f8f91fd5c3db6591",
        "type": "tabs",
        "children": [
          {
            "id": "eab7913bcd019e86",
            "type": "leaf",
            "state": {
              "type": "file-explorer",
              "state": {
                "sortOrder": "alphabeticalReverse"
              }
            }
          },
          {
            "id": "88e44a922c0882da",
            "type": "leaf",
            "state": {
              "type": "search",
              "state": {
                "query": "tag:#VAE",
                "matchingCase": true,
                "explainSearch": false,
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical"
              }
            }
          },
          {
            "id": "c82ed47fe7217caa",
            "type": "leaf",
            "state": {
              "type": "starred",
              "state": {}
            }
          }
        ]
      }
    ],
    "direction": "horizontal",
    "width": 405.5
  },
  "right": {
    "id": "2d96c24d50c19245",
    "type": "split",
    "children": [
      {
        "id": "fb187973d1ea0ad5",
        "type": "tabs",
        "children": [
          {
            "id": "90f763b8c455e675",
            "type": "leaf",
            "state": {
              "type": "backlink",
              "state": {
                "file": "05_speech_recognition/(2021) W2V-BERT, Combining Contrastrive learning and Masked Language Modeling For Self-Supervised Speech Pre-Training.md",
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical",
                "showSearch": false,
                "searchQuery": "",
                "backlinkCollapsed": false,
                "unlinkedCollapsed": true
              }
            }
          },
          {
            "id": "7f3b9ad5c3071af9",
            "type": "leaf",
            "state": {
              "type": "outgoing-link",
              "state": {
                "file": "05_speech_recognition/(2021) W2V-BERT, Combining Contrastrive learning and Masked Language Modeling For Self-Supervised Speech Pre-Training.md",
                "linksCollapsed": false,
                "unlinkedCollapsed": true
              }
            }
          },
          {
            "id": "464d2c7bfeabec63",
            "type": "leaf",
            "state": {
              "type": "tag",
              "state": {
                "sortOrder": "frequency",
                "useHierarchy": true
              }
            }
          },
          {
            "id": "53fd65d16d68f55e",
            "type": "leaf",
            "state": {
              "type": "outline",
              "state": {
                "file": "05_speech_recognition/(2021) W2V-BERT, Combining Contrastrive learning and Masked Language Modeling For Self-Supervised Speech Pre-Training.md"
              }
            }
          }
        ],
        "currentTab": 2
      }
    ],
    "direction": "horizontal",
    "width": 300
  },
  "left-ribbon": {
    "hiddenItems": {
      "canvas:新建白板": false,
      "templater-obsidian:Templater": false,
      "switcher:打开快速切换": false,
      "graph:查看关系图谱": false,
      "daily-notes:打开/创建今天的日记": false,
      "templates:插入模板": false,
      "command-palette:打开命令面板": false
    }
  },
  "active": "eab7913bcd019e86",
  "lastOpenFiles": [
    "(2018) Nerual Discrete Representation Learning.md",
    "05_speech_recognition/(2021) W2V-BERT, Combining Contrastrive learning and Masked Language Modeling For Self-Supervised Speech Pre-Training.md",
    "01_natural_language_processing/(2022) Unsupervised Dense Information Retrieval with Contrastive Learning.md",
    "01_natural_language_processing/(2022) Sequence Parallelism, Long Sequence Training from System Perspective.md",
    "99_attachment/1711.00937v2.pdf",
    "99_attachment/kmeans_vector_quantizer 1.py",
    "99_attachment/gumbel_vector_quantizer 1.py",
    "99_attachment/2108.06209v2 1.pdf",
    "10_books/(2023) 元宇宙时代超高清视音频技术白皮书.md",
    "05_speech_recognition/(2022) Japanese ASR-Robust Pre-trained Language Model with Pseudo-Error Sentence Generated by Grapheme-Phoneme Conversion.md",
    "05_speech_recognition/(2022) SpeechNet, Weekly Supervised, End-to-End Speech Recognition at Industrial Scale.md",
    "03_multimodal/(2021) Learning Transferable Visual Models From Natural Language Processing.md",
    "03_multimodal/(2022) Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models.md",
    "03_multimodal/(2023) Visual ChatGPT, Talking, Drawing and Editing with Visual Foundation Models.md",
    "02_computer_vision/(2018) high-resolution image synthesis and semantic manipulation with conditional GANs.md",
    "02_computer_vision/(2019) Everybody Dance Now.md",
    "02_computer_vision/(2020) StartGAN v2.md",
    "02_computer_vision/(2021) Learning Transferable Visual Models From Natural Language Processing.md",
    "02_computer_vision/(2023) GLIGEN, Open-Set Grounded Text-to-Image Generation.md",
    "10_books/(2019) The Book of Why.md",
    "99_attachment/元宇宙时代超高清视音频技术白皮书-1.pdf",
    "10_books/(2018) Tutorial on Deep Generative Models.md",
    "10_books/(2018) Clean Code in Python Refactor your legacy code base.md",
    "90_tools/docker.md",
    "90_tools/GPU.md",
    "01_natural_language_processing/(2023) LLaMA, Open and Efficient Foundation Language Models.md",
    "99_attachment/Pasted image 20230331180253.png",
    "99_attachment/Pasted image 20230331180238.png",
    "01_natural_language_processing/(2022) GPT-NeoX-20B, An Open-Source Autoregressive Language Model.md",
    "(2022) GPT-NeoX-20B, An Open-Source Autoregressive Language Model.md",
    "99_attachment/GPT-NeoX-20B 1.pdf",
    "01_natural_language_processing/(2020) GPT3, Language Models are Few-Shot Learners.md",
    "01_natural_language_processing/(2023) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2023).pdf.md",
    "01_natural_language_processing/(2022) Instruct GPT.md",
    "99_attachment/Datasheet for the pile.pdf",
    "99_attachment/The Pile, An 800GB Dataset of Diverse Text for Lange Modeling.pdf",
    "99_attachment/GPT-NeoX-20B.pdf",
    "99_attachment/Early experiments with GPT-4 1.pdf",
    "99_attachment/Pasted image 20230328134959.png",
    "99_attachment/Pasted image 20230328134837.png",
    "99_attachment/Pasted image 20230328134819.png",
    "99_attachment/Pasted image 20230328134508.png",
    "99_attachment/Pasted image 20230328134451.png",
    "99_attachment/Pasted image 20230328134244.png",
    "99_attachment/Pasted image 20230328134044.png",
    "99_attachment/Pasted image 20230328134020.png"
  ]
}