介绍目前合成数据所面临的问题。

## 主要问题
---
- 生成/合成的数据不太真实，可靠性较低。(生成的数据有的像真实数据，也有很多不像真实数据)
- 生成/合成的数据多样性较差，或者说有效的多样性较差。(虽然生成训练数据没有的数据，但是也远离实现场景)
- 生成/合成的数据有效性来源不太确定，突破真实困境较难。
	- 是否可以生成真实的数据集之外的数据，为重中之重。
	- 数据集以内的知识组合或者连续化，可能因为算法较弱等原因有效。

## 合成种类
---
- 将另一个数据集迁移至当前数据集
	- 虚拟场景街道的数据集→真实场景街道的数据集
- 知识蒸馏的方法：（需要考虑ChatGPT产生的数据）
- 控制生成的数据属性：提供文本规范和示例，来控制生成数据的属性。

## 主要工作
---
- [[(2021) Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization]]
	- 主要目的：使用GPT3生成Medical dialogue summarization的数据，提高现有模型效果
	- 主要操作：①通过多个结果里面选则最好的，优化GPT3的性能。②通过GPT3生成的伪数据，再本地预训练模型进行Finetune。
	- 最终效果：210条真实数据生成的6400伪数据，约等于6400条真实数据效果。
- [[(2022) Symbolic Knowledge Distillation, from General Language MOdels to Commonsense Models]]
	- 主要目的：公共知识提取
	- 主要操作：使用GPT3生成数据，在训练一个filter的模型（人工标注好坏1万）
	- 最终效果：好于GPT3很多
- [[(2022) CLASP, Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing]]
	- 主要目的：数据增广Semantic parsing
	- 主要操作：使用大模型进行数据增广处理
	- 最终效果：提升2~5个点左右
- [[(2023) SODA, Million-scale Dialogue Distillation with Social Commonsense Contextualization]]
	- 主要目的：从InstructGPT, 使用公共知识库Distill 1.5M 的对话数据集
	- 主要操作：①从公共知识库中挑选数据②知识通过instructGPT转成简单句③简单句通过instructGPT转成对话句
	- 最终效果：远远高于现有方法
- [[(2023) Language Models are Realistic tabular Data Generators]]
	- 主要目的：Tabular Data 生成。
	- 主要操作：制作Tabular Data生成模型，训练生成模型①Tabular转成每一行一个文本（基于模板）②文本Finetune一个语言模型。测试生成模型，给多行文本（只给前部分）输入至模型，模型补全后半部分。将后半部分文本转化成Tabular（转化比较简单，模板化）
	- 最终效果：1~2个点的提升比较多，少量到5个点左右。
- [[(2023) Does Synthetic Data Generation of LLMs Help Clinical Text Mining]]
	- 主要目的：ChatGPT在医学NER等表现不好，利用ChatGPT生成数据进行Finetune
	- 主要操作：利用ChatGPT生成好的Prompt，利用生成好的Prompt生成数据，进行Finetune训练
	- 最终效果：比Zero-shot好好很多，但是比真实数据差很多。