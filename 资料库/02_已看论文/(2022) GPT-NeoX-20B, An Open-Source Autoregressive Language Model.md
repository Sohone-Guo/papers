#Tag_自然语言处理 
# Information
---
- [[eleuther.ai]]

# Mainly Idea
---
- 3.15M tokens or 1538 contexts of 2048 toens each
- tensor parallel size of 2, and a pipline parallel size of 4

# Reference
---
- dataset: [[(2020) The Pile, An 800GB Dataset of Diverse Text for Language Modeling]]
- github: [EleutherAI/gpt-neox: An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library. (github.com)](https://github.com/EleutherAI/gpt-neox)
- huggingface: [EleutherAI/gpt-neox-20b · Hugging Face](https://huggingface.co/EleutherAI/gpt-neox-20b?text=Here+we+have+4+functions%2C+the+function+list+are+%7Bcopy_func%2C+main_func%2C+remove_func%2C+repeat_func%7D.+The+first+function+in+the+list+is+%7Bcopy_func%7D.+What+is+the+last+function%3F+The+last+function+in+the+list+is+%7Bmain_func%7D.+The+answer+is+wrong.+What+is+the+last+function%3F)

# Attachment
---
![[GPT-NeoX-20B.pdf]]