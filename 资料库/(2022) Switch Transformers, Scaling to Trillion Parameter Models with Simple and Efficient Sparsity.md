Tags: #Tag_基础理论 #Tag_MoE
# Information
---


# Mainly Idea
---


# Question
---


# Reference
---
- [(10) Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity - YouTube](https://www.youtube.com/watch?v=iAR8LkkMMIM)
- [(10) CS25 I Stanford Seminar 2022 - Mixture of Experts (MoE) paradigm and the Switch Transformer - YouTube](https://www.youtube.com/watch?v=U8J32Z3qV8s)

# Attachment
---
![[Switch Transformers.pdf]]