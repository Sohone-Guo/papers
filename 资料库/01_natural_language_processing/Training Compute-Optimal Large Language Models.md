# Information
---
-  [[DeepMind]]

# Mainly Idea
---
Current large language models are significantly under-trained. The model size and the number of tokens should be scaled equally.
- Chinchilla (70B), 4x datasets (better)
- Gopher (280B), 1x datasets
计算，模型大小和数据大小的关系，资源有限的前提下，参数量和数据量的关系

相关论文：[[Scaling Laws for Neural Language Models]]

# Reference
---
- [Chinchilla Explained: Compute-Optimal Massive Language Models - YouTube](https://www.youtube.com/watch?v=PZXN7jm9IC0)

# Attachment
---
![[Training Compute-Optimal Large Language Models.pdf]]