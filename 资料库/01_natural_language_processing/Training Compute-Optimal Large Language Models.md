# Information
---
-  [[DeepMind]]

# Mainly Idea
---
Current large language models are significantly under-trained. The model size and the number of tokens should be scaled equally.
- Chinchilla (70B), 4x datasets (better)
- Gopher (280B), 1x datasets

# Reference
---


# Attachment
---
![[Training Compute-Optimal Large Language Models.pdf]]